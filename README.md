# 🖼️🔤 Vision-Language Models (VLMs)

This repository gathers concepts, research notes, and references on **Vision-Language Models (VLMs)** — AI systems that jointly learn from **visual and textual data** to enable multimodal understanding and reasoning. 

---

## 🔍 Motivation

The integration of vision and language is one of the most promising directions in AI. 
VLMs enable models to **connect visual perception with natural language**, unlocking powerful applications: 
- Image and video captioning 
- Visual Question Answering (VQA) 
- Multimodal retrieval (text-to-image / image-to-text) 
- Generative AI for text-to-image and video synthesis 

In domains like **physical security and surveillance**, vision-language alignment can help create more **interpretable, explainable, and responsible AI systems**. 

---

## 🏗️ Key Concepts

This repo documents: 
- **Contrastive learning** (e.g., CLIP) for joint vision-text representation 
- **Cross-attention architectures** for aligning modalities 
- **Multimodal transformers** for generative tasks 
- **Evaluation challenges**: robustness, interpretability, and bias 
- **Efficient deployment** of VLMs on constrained systems 

---

## 📚 Publications & References

Relevant works include both community references and my own contributions: 

- Radford et al. **CLIP: Connecting Vision and Language** – *ICML 2021* 
- Li et al. **BLIP: Bootstrapping Language-Image Pre-training** – *ICML 2022* 
- OpenAI, **GPT-4V: Multimodal Large Language Models** – *2023* 

**My related work:** 
- Héritier et al., [**EXaM: Unsupervised Concept-Based Representation Learning to Better Explain Models in Vision Tasks**](https://scholar.google.com/citations?view_op=view_citation&hl=fr&user=OBIkP1AAAAAJ&citation_for_view=OBIkP1AAAAAJ:Tyk-4Ss8FVUC) – *XAI4CV CVPR Workshop 2025, Spotlight* 


➡️ [Google Scholar Profile](https://scholar.google.com/citations?user=OBIkP1AAAAAJ&hl=fr&oi=ao) for full list of publications. 

---

## 🚀 Repository Content

⚠️ *Note: Industrial code is not shared due to IP restrictions. This repo serves as a research hub and reference point.* 

---

## 📫 Contact

- [LinkedIn](https://www.linkedin.com/in/maguelonne-heritier-03ba9bb/)
- [Google Scholar](https://scholar.google.com/citations?user=OBIkP1AAAAAJ&hl=fr&oi=ao)
